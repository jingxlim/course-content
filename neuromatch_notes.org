#+STARTUP: entitiespretty

* General reflections
- there's no one tool for one problem
- practioners have preferences
* 20JUL14 Neuromatch Day 1 QnA
- If all models are wrong, then there's no point to prove falsify models
- if it is a bernouli proccess, the ISI is exponential; but the reverse is not true (i.e. if the ISI is exponential, it might not be due to a bernouli process)
- the "typical" neuron is often not the most representative. It's the neuron that fit most well with theory that people read about.
- asking representation questions ('what' models): you can't be sure that the input you give to the neuron are the correct features
- neurons that don't respond the way we expect them to respond, we think are just noise -- this might not be correct
- whenever we find that a neuron represents something, we give it a name and forget about the representative space hasn't been fully explored
* 20JUL15 Neuromatch Day 2 QnA
- just lumping everything unexplainable as noise is dishonest
- you also need to account for noise, to point it to e.g. a random process (e.g. chemical processes, quantum fluctuations in the retina)
- stochastic resonsance is one reason why noise might be helpful
  - white noise in auditory system will help when noise is just below percetual threshold (stochastic facilitation)
- noise is in built into the hardware/implementation; physics, molecules, flow of liquids (in vestibular systems) are noisy
- variational autoencoders: increasing noise builds robustness into adverserial attacks
- ensemble learning, deep learning, or gaussian processes are very flexible and don't require any knowledge about the system you are modelling; that might give you great predictions but not any understanding
- what is a good r^2 values to publish? science is as much as sociological enterprize as it is a logical exercise. in some fields that always deal with noisy phenomenon or if the field is in its infancy, even low r^2 values is interesting.
- How might we effectively use Occam's Razor to narrow down what models are considered "good" or valid in explaining relationships in the data - esp. in cases where it's not clear which models are "simpler"? Is there a more rigorous way of understanding which models make fewer assumptions?
  - there's some theoretical work behind this; you might be able to do it exactly with generative models
  - minimum prescription length: smallest possible pseudocode
  - ensemble model comparisons
  - this paper was mentioned here but not sure why: [[http://www.psy.vanderbilt.edu/courses/psy236/Motion/Motion.17March/Borst(NatNeuro2000).pdf][Models of motion detection by Alexander Borst]]
- at some point, people trust the theory better than the data! e.g. HH model
* 20JUL15 Neuromatch Day 3 QnA
- Cross validation, because of multiple splits has a lot of computational cost; AIC you just need to do one run
- Cross validation, because of the splits, you also only use part of the training set
- MLE is more flexible which can makes use of different structures of noise
- MLE you need to write to write out explicitly, you need to say what the nosie to look like; MSE you don't have to make what could be an assumption
- PCA is more similar to MSE in that it is maximizing variance, and there's no need to consider probabilities
- bootstraping is problematic if you have a small dataset where there are not many ways to resample data
- however, bootstraping doesn't make any assumptions
* 20JUL16 Neuromatch Day 4 Outro
- GLMs are not flexible enough to solve arbitrary nonlinear problems. For example, the Poisson-GLM was fixed to be log-linear.
- However, this does not mean you shouldn't try it for nonlinear problems! Most nonlinaer problems have a large linear component. GLM is very practical. GLM will be able to predict most of it.
- since it is linear, it's practical and very interpretable
- you can also introduce nonlinear features
* 20JUL16 Neuromatch Day 4 QnA
- you typically first start with L2, unless you have aprior reason that L1 is better (e.g. not all neurons should encode some experimental parameter)
- you have lots of non-linearity to choose from, but the nice property of having a convex solution might break
- dimensionality reduction is applying a non-linear transformation to your feature space
* 20JUL16 Neuromatch Day 5 QnA
- dimensionality reduction can be used for visualization, denoising and can also be thought about a fundamental bnuilding block of cognition
- Carsen stringer: t-sne and Umap is good for neural data
- differnet methods have different requirements and make different assumptions
- you can use PCs from one half of the data and project to the other half of the data to choose a cutoff for the number of PCs
- how to choose perplexity?
  - preservation of local and global distances
  - e.g. check that if two neural acitivty are correlated, you want to see that these points are close together
  - perplexity should scale with number of data points
- intuition behind t-SNE: map correlation matrix to a 2D space, using a non-linear transformation
  - this helps with keeping correlated neurons together
  - initialisation is important; there's some stochasticity
- Suite2P no longer does PCA first before NMF. It now bins the data first and then looks for sparse events.
- ICA is based on the fact that you have the same signal with different amplitudes at various "sites"
- every dimensionality reduction technique has its own definition about what is signal and what is noise
- people use dimensionality reduction of neural activity as input to BCI, rarely do we use single neuron activity. Sara thinks that the brain uses ensembles for computation, not single neuron.
- What are the open questions in the dimensionality reduction field?
  - we don't know if there are  intrinsic dimensions to be discovered
  - noisy data. as long as you don't know what the ground truth is, you won't know that your method is failing
  - we don't know what is noise
    - noise is something that we cannot explain
    - noise is not correlated to anything else across the population
- you can use trial averaged data to get your eigenvectors. you can then project your noisy trial data onto the trial average to do denoising.
- isomap is able to recover curved surfaces (e.g. cinnamon roll, S-shaped)
- How would you know if your data is nonlinear?
  - make lots of 2D plots
  - think about the external correlates
  - look at the residuals
- Advice:
  - Sara: always plot your data, always start linear. wrong hypothesis will open other avenues for exploration.
  - Byron: don't just take any off-the-shelf dimensionality reduction technique and apply it to your data. test your understanding of the technique with simulate data where you know the ground truth
  - Carsen: there are other kinds of dimensionality reduction techqniues e.g. reduced rank regression -- shared dimensionality to an external correlate
